#!/usr/bin/env python3
"""
üî¨ FeatureManager - —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏

–ì–µ–Ω–µ—Ä–∞—Ü–∏—è, –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
"""

import pickle
import hashlib
from typing import Dict, List, Optional, Any
from pathlib import Path
import pandas as pd
import numpy as np
from datetime import datetime

from ..utils.config import Config
from ..utils.logger import Logger
from ..utils.validators import FeatureValidator, ValidationError
from .feature_engineer import FeatureEngineer

class FeatureManager:
    """
    –ú–µ–Ω–µ–¥–∂–µ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    
    –û—Ç–≤–µ—á–∞–µ—Ç –∑–∞:
    - –ì–µ–Ω–µ—Ä–∞—Ü–∏—é —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
    - –°–æ–∑–¥–∞–Ω–∏–µ multi-timeframe –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    - –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    - –í–∞–ª–∏–¥–∞—Ü–∏—é –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    """
    
    def __init__(self, config: Config):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        
        Args:
            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã
        """
        self.config = config
        self.engineer = FeatureEngineer()
        self.logger = Logger('FeatureManager', level=config.log_level)
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        self.cache_root = Path(config.cache_root) / 'features'
        self.cache_root.mkdir(parents=True, exist_ok=True)
        
        # –ö—ç—à –≤ –ø–∞–º—è—Ç–∏
        self._memory_cache = {}
        self._cache_metadata = {}
    
    def _generate_cache_key(self, data_hash: str, feature_config: Dict[str, Any]) -> str:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–∞ –∫—ç—à–∞ –¥–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        
        Args:
            data_hash: –•–µ—à –¥–∞–Ω–Ω—ã—Ö
            feature_config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            
        Returns:
            –ö–ª—é—á –∫—ç—à–∞
        """
        # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä–æ–∫—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        config_str = str(sorted(feature_config.items()))
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ö–µ—à –¥–∞–Ω–Ω—ã—Ö –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        combined = f"{data_hash}_{config_str}"
        
        # –°–æ–∑–¥–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ö–µ—à
        return hashlib.md5(combined.encode()).hexdigest()
    
    def _calculate_data_hash(self, data: Dict[str, pd.DataFrame]) -> str:
        """
        –†–∞—Å—á–µ—Ç —Ö–µ—à–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
        
        Args:
            data: –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –ø–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞–º
            
        Returns:
            –•–µ—à –¥–∞–Ω–Ω—ã—Ö
        """
        # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä–æ–∫—É —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –¥–∞–Ω–Ω—ã—Ö
        data_info = []
        for timeframe, df in data.items():
            data_info.append(f"{timeframe}:{len(df)}:{df.index.min()}:{df.index.max()}")
        
        data_str = "|".join(sorted(data_info))
        return hashlib.md5(data_str.encode()).hexdigest()
    
    def generate_features(self, data: Dict[str, pd.DataFrame], 
                         feature_config: Optional[Dict] = None) -> pd.DataFrame:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        
        Args:
            data: –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –ø–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞–º
            feature_config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            
        Returns:
            DataFrame —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
        """
        self.logger.info("üî¨ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–∞
        if feature_config is None:
            feature_config = {
                'include_technical_indicators': self.config.features.include_technical_indicators,
                'include_multi_timeframe': self.config.features.include_multi_timeframe,
                'include_lag_features': self.config.features.include_lag_features,
                'include_rolling_features': self.config.features.include_rolling_features,
                'lag_windows': self.config.features.lag_windows,
                'roll_windows': self.config.features.roll_windows
            }
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        data_hash = self._calculate_data_hash(data)
        cache_key = self._generate_cache_key(data_hash, feature_config)
        
        cached_features = self._load_from_cache(cache_key)
        if cached_features is not None:
            self.logger.info("‚úÖ –ü—Ä–∏–∑–Ω–∞–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ –∫—ç—à–∞")
            return cached_features
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        self.logger.info("üîß –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (15m)
            df_15m = data.get('15m')
            if df_15m is None:
                raise ValueError("–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –¥–∞–Ω–Ω—ã–µ 15m")
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –±–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ 15m
            if feature_config.get('include_technical_indicators', True):
                self.logger.info("   üìä –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã")
                df_15m_all = self.engineer.create_all_features(df_15m)
            else:
                df_15m_all = df_15m.copy()
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º multi-timeframe –ø—Ä–∏–∑–Ω–∞–∫–∏
            if feature_config.get('include_multi_timeframe', True):
                self.logger.info("   üîÑ Multi-timeframe –ø—Ä–∏–∑–Ω–∞–∫–∏")
                df_multi = self._create_multi_timeframe_features(data)
                
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
                df_full = pd.concat([df_15m_all, df_multi], axis=1)
            else:
                df_full = df_15m_all
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º inf/NaN –∑–Ω–∞—á–µ–Ω–∏—è
            if self.config.features.handle_inf_nan:
                self.logger.info("   üßπ –û—á–∏—Å—Ç–∫–∞ inf/NaN –∑–Ω–∞—á–µ–Ω–∏–π")
                df_full = self._clean_features(df_full)
            
            # –í–∞–ª–∏–¥–∏—Ä—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
            if self.config.features.validate_features:
                self.logger.info("   ‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
                FeatureValidator.validate_feature_quality(df_full)
                FeatureValidator.validate_feature_types(df_full)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
            self._save_to_cache(cache_key, df_full, data_hash, feature_config)
            
            self.logger.info(f"‚úÖ –ü—Ä–∏–∑–Ω–∞–∫–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã: {df_full.shape}")
            return df_full
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")
            raise
    
    def _create_multi_timeframe_features(self, data: Dict[str, pd.DataFrame]) -> pd.DataFrame:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ multi-timeframe –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        
        Args:
            data: –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –ø–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞–º
            
        Returns:
            DataFrame —Å multi-timeframe –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
        """
        df_15m = data.get('15m')
        df_1h = data.get('1h')
        df_4h = data.get('4h')
        df_1d = data.get('1d')
        
        # –°–æ–∑–¥–∞–µ–º multi-timeframe –ø—Ä–∏–∑–Ω–∞–∫–∏
        df_multi = self.engineer.create_multi_timeframe_features(df_15m, df_1h, df_4h, df_1d)
        
        # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ multi-TF –ø—Ä–∏–∑–Ω–∞–∫–∏
        multi_cols = [
            col for col in df_multi.columns 
            if any(prefix in col for prefix in ['rsi_1h', 'trend_1h', 'macd_4h', 'adx_4h', 'trend_1d'])
        ]
        
        if multi_cols:
            df_multi = df_multi[multi_cols]
        else:
            # –ï—Å–ª–∏ –Ω–µ—Ç multi-TF –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, —Å–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π DataFrame
            df_multi = pd.DataFrame(index=df_15m.index)
        
        return df_multi
    
    def _clean_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        –û—á–∏—Å—Ç–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –æ—Ç inf/NaN –∑–Ω–∞—á–µ–Ω–∏–π –∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        
        Args:
            df: DataFrame —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
            
        Returns:
            –û—á–∏—â–µ–Ω–Ω—ã–π DataFrame
        """
        # –£–¥–∞–ª—è–µ–º –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        constant_features = []
        for col in df.columns:
            if df[col].nunique() <= 1:
                constant_features.append(col)
        
        if constant_features:
            self.logger.info(f"   üóëÔ∏è –£–¥–∞–ª—è–µ–º {len(constant_features)} –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
            df = df.drop(columns=constant_features)
        
        # –ó–∞–º–µ–Ω—è–µ–º inf –Ω–∞ NaN
        df = df.replace([np.inf, -np.inf], np.nan)
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è
        df = df.fillna(method='ffill').fillna(method='bfill')
        
        # –ï—Å–ª–∏ –æ—Å—Ç–∞–ª–∏—Å—å NaN, –∑–∞–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏
        df = df.fillna(0)
        
        return df
    
    def _load_from_cache(self, cache_key: str) -> Optional[pd.DataFrame]:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –∫—ç—à–∞
        
        Args:
            cache_key: –ö–ª—é—á –∫—ç—à–∞
            
        Returns:
            DataFrame —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –∏–ª–∏ None
        """
        if not self.config.features.cache_features:
            return None
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à –≤ –ø–∞–º—è—Ç–∏
        if cache_key in self._memory_cache:
            metadata = self._cache_metadata.get(cache_key, {})
            if self._is_cache_valid(metadata):
                return self._memory_cache[cache_key]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à –Ω–∞ –¥–∏—Å–∫–µ
        cache_path = self.cache_root / f"{cache_key}.pkl"
        if cache_path.exists():
            try:
                with open(cache_path, 'rb') as f:
                    cached_data = pickle.load(f)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –∫—ç—à–∞
                if isinstance(cached_data, dict) and 'features' in cached_data:
                    metadata = cached_data.get('metadata', {})
                    if self._is_cache_valid(metadata):
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ø–∞–º—è—Ç—å
                        self._memory_cache[cache_key] = cached_data['features']
                        self._cache_metadata[cache_key] = metadata
                        return cached_data['features']
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫—ç—à–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")
        
        return None
    
    def _save_to_cache(self, cache_key: str, features: pd.DataFrame, 
                      data_hash: str, feature_config: Dict[str, Any]):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ –∫—ç—à
        
        Args:
            cache_key: –ö–ª—é—á –∫—ç—à–∞
            features: DataFrame —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
            data_hash: –•–µ—à –¥–∞–Ω–Ω—ã—Ö
            feature_config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        """
        if not self.config.features.cache_features:
            return
        
        cache_path = self.cache_root / f"{cache_key}.pkl"
        
        try:
            # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫—ç—à–∞
            metadata = {
                'created_at': datetime.now().isoformat(),
                'data_hash': data_hash,
                'feature_config': feature_config,
                'shape': features.shape,
                'columns': list(features.columns),
                'features_hash': self._calculate_features_hash(features)
            }
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ø–∞–º—è—Ç—å
            self._memory_cache[cache_key] = features
            self._cache_metadata[cache_key] = metadata
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞ –¥–∏—Å–∫
            cache_data = {
                'features': features,
                'metadata': metadata
            }
            
            with open(cache_path, 'wb') as f:
                pickle.dump(cache_data, f)
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫—ç—à–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")
    
    def _is_cache_valid(self, metadata: Dict[str, Any]) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ –∫—ç—à–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        
        Args:
            metadata: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫—ç—à–∞
            
        Returns:
            True –µ—Å–ª–∏ –∫—ç—à –≤–∞–ª–∏–¥–µ–Ω
        """
        if not metadata:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ä–µ–º—è —Å–æ–∑–¥–∞–Ω–∏—è
        created_at = metadata.get('created_at')
        if created_at:
            try:
                created_time = datetime.fromisoformat(created_at)
                age = datetime.now() - created_time
                if age.total_seconds() > self.config.cache_ttl:
                    return False
            except:
                return False
        
        return True
    
    def _calculate_features_hash(self, features: pd.DataFrame) -> str:
        """
        –†–∞—Å—á–µ—Ç —Ö–µ—à–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏
        
        Args:
            features: DataFrame —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
            
        Returns:
            –•–µ—à –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        """
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–µ –∏ –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ö–µ—à–∞
        sample_features = pd.concat([features.head(10), features.tail(10)])
        features_str = sample_features.to_string()
        return hashlib.md5(features_str.encode()).hexdigest()
    
    def validate_features_for_model(self, features: pd.DataFrame, 
                                  expected_features: List[str]) -> bool:
        """
        –í–∞–ª–∏–¥–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏
        
        Args:
            features: DataFrame —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
            expected_features: –°–ø–∏—Å–æ–∫ –æ–∂–∏–¥–∞–µ–º—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            
        Returns:
            True –µ—Å–ª–∏ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤–∞–ª–∏–¥–Ω—ã
        """
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            FeatureValidator.validate_features(features, expected_features)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            FeatureValidator.validate_feature_quality(features)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            FeatureValidator.validate_feature_types(features)
            
            return True
            
        except ValidationError as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")
            return False
    
    def get_feature_info(self, features: pd.DataFrame) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö
        
        Args:
            features: DataFrame —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö
        """
        info = {
            'shape': features.shape,
            'total_features': len(features.columns),
            'numeric_features': len(features.select_dtypes(include=[np.number]).columns),
            'memory_usage': features.memory_usage(deep=True).sum(),
            'missing_values': features.isnull().sum().sum(),
            'inf_values': np.isinf(features.select_dtypes(include=[np.number])).sum().sum(),
            'feature_types': features.dtypes.value_counts().to_dict()
        }
        
        # –ê–Ω–∞–ª–∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        feature_categories = {
            'technical_indicators': [col for col in features.columns if any(indicator in col.lower() for indicator in ['rsi', 'macd', 'ema', 'sma', 'bollinger', 'stochastic'])],
            'lag_features': [col for col in features.columns if 'lag' in col.lower()],
            'rolling_features': [col for col in features.columns if 'rolling' in col.lower() or 'roll' in col.lower()],
            'multi_timeframe': [col for col in features.columns if any(tf in col for tf in ['_1h', '_4h', '_1d'])],
            'price_features': [col for col in features.columns if any(price in col.lower() for price in ['open', 'high', 'low', 'close', 'volume'])],
            'other': []
        }
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º "–¥—Ä—É–≥–∏–µ" –ø—Ä–∏–∑–Ω–∞–∫–∏
        all_categorized = set()
        for category_features in feature_categories.values():
            all_categorized.update(category_features)
        
        feature_categories['other'] = [col for col in features.columns if col not in all_categorized]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        for category, category_features in feature_categories.items():
            info[f'{category}_count'] = len(category_features)
        
        return info
    
    def select_features(self, features, target: pd.Series, 
                       method: str = 'permutation', threshold: float = 0.01, 
                       remove_correlated: bool = True, correlation_threshold: float = 0.95,
                       n_features: Optional[int] = None, feature_names: Optional[List[str]] = None) -> List[str]:
        """
        –û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        
        Args:
            features: DataFrame —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
            target: Series —Å —Ç–∞—Ä–≥–µ—Ç–æ–º
            method: –ú–µ—Ç–æ–¥ –æ—Ç–±–æ—Ä–∞ ('permutation', 'correlation', 'mutual_info', 'combined', 'recursive')
            threshold: –ü–æ—Ä–æ–≥ –¥–ª—è –æ—Ç–±–æ—Ä–∞
            remove_correlated: –£–¥–∞–ª—è—Ç—å –ª–∏ –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            correlation_threshold: –ü–æ—Ä–æ–≥ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
            n_features: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ—Ç–±–æ—Ä–∞ (–¥–ª—è recursive –º–µ—Ç–æ–¥–∞)
            
        Returns:
            –°–ø–∏—Å–æ–∫ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        """
        self.logger.info(f"üéØ –û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –º–µ—Ç–æ–¥–æ–º: {method}")
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º numpy –º–∞—Å—Å–∏–≤ –≤ DataFrame –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if isinstance(features, np.ndarray):
            if feature_names is None:
                feature_names = [f'feature_{i}' for i in range(features.shape[1])]
            features_df = pd.DataFrame(features, columns=feature_names)
        else:
            features_df = features
            feature_names = list(features.columns)
        
        # –°–Ω–∞—á–∞–ª–∞ —É–¥–∞–ª—è–µ–º —Å–∏–ª—å–Ω–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        if remove_correlated:
            self.logger.info("üîó –£–¥–∞–ª–µ–Ω–∏–µ –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
            features_clean = self._remove_correlated_features(features_df, correlation_threshold)
            self.logger.info(f"   –û—Å—Ç–∞–ª–æ—Å—å {len(features_clean.columns)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ {len(features_df.columns)}")
        else:
            features_clean = features_df
        
        # –í—ã–±–∏—Ä–∞–µ–º –º–µ—Ç–æ–¥ –æ—Ç–±–æ—Ä–∞
        if method == 'permutation':
            selected = self._select_features_permutation(features_clean, target, threshold)
        elif method == 'correlation':
            selected = self._select_features_correlation(features_clean, target, threshold)
        elif method == 'mutual_info':
            selected = self._select_features_mutual_info(features_clean, target, threshold)
        elif method == 'combined':
            selected = self._select_features_combined(features_clean, target, threshold)
        elif method == 'recursive':
            if n_features is None:
                n_features = min(50, len(features_clean.columns) // 2)
            selected = self._select_features_recursive(features_clean, target, n_features)
        elif method == 'stability':
            selected = self._select_features_stability(features_clean, target, threshold)
        else:
            raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –º–µ—Ç–æ–¥ –æ—Ç–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {method}")
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ–Ω—è—Ç–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        if isinstance(features, np.ndarray):
            # –ï—Å–ª–∏ –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –±—ã–ª–∏ numpy –º–∞—Å—Å–∏–≤–æ–º, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è
            selected_real_names = []
            for feat in selected:
                if feat in feature_names:
                    selected_real_names.append(feat)
                elif feat.startswith('feature_'):
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω–¥–µ–∫—Å –∏–∑ feature_X
                    try:
                        idx = int(feat.split('_')[1])
                        if idx < len(feature_names):
                            selected_real_names.append(feature_names[idx])
                    except (ValueError, IndexError):
                        selected_real_names.append(feat)
            return selected_real_names
        else:
            return selected
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –ø–æ–Ω—è—Ç–Ω—ã–º–∏ –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏
        if selected:
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∫–∞–∫ –ø—Ä–∏–º–µ—Ä—ã
            examples = selected[:5]
            if len(selected) > 5:
                examples_str = ", ".join(examples) + f" ... –∏ –µ—â–µ {len(selected)-5}"
            else:
                examples_str = ", ".join(examples)
            
            self.logger.info(f"‚úÖ –û—Ç–æ–±—Ä–∞–Ω–æ {len(selected)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {examples_str}")
        else:
            self.logger.info(f"‚úÖ –û—Ç–æ–±—Ä–∞–Ω–æ {len(selected)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        
        return selected
    
    def _select_features_permutation(self, features: pd.DataFrame, target: pd.Series, 
                                   threshold: float) -> List[str]:
        """
        –û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –º–µ—Ç–æ–¥–æ–º permutation importance
        
        Args:
            features: DataFrame —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
            target: Series —Å —Ç–∞—Ä–≥–µ—Ç–æ–º
            threshold: –ü–æ—Ä–æ–≥ –≤–∞–∂–Ω–æ—Å—Ç–∏
            
        Returns:
            –°–ø–∏—Å–æ–∫ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        """
        from sklearn.inspection import permutation_importance
        from sklearn.ensemble import RandomForestRegressor
        
        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        model = RandomForestRegressor(
            n_estimators=200,  # –ë–æ–ª—å—à–µ –¥–µ—Ä–µ–≤—å–µ–≤ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            max_depth=10,      # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≥–ª—É–±–∏–Ω—É
            min_samples_split=10,  # –ú–∏–Ω–∏–º—É–º —Å—ç–º–ø–ª–æ–≤ –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
            min_samples_leaf=5,    # –ú–∏–Ω–∏–º—É–º —Å—ç–º–ø–ª–æ–≤ –≤ –ª–∏—Å—Ç–µ
            random_state=42,
            n_jobs=-1  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ —è–¥—Ä–∞
        )
        model.fit(features, target)
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º permutation importance —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        perm_importance = permutation_importance(
            model, features, target, 
            n_repeats=5,   # –£–º–µ–Ω—å—à–∞–µ–º –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
            random_state=42,
            n_jobs=-1
        )
        
        # –û—Ç–±–∏—Ä–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤—ã—à–µ –ø–æ—Ä–æ–≥–∞ —Å —É—á–µ—Ç–æ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è
        selected_features = []
        for i, col in enumerate(features.columns):
            mean_importance = perm_importance.importances_mean[i]
            std_importance = perm_importance.importances_std[i]
            
            # –ü—Ä–∏–∑–Ω–∞–∫ –≤–∞–∂–µ–Ω –µ—Å–ª–∏ —Å—Ä–µ–¥–Ω—è—è –≤–∞–∂–Ω–æ—Å—Ç—å –≤—ã—à–µ –ø–æ—Ä–æ–≥–∞ –ò —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –Ω–µ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–µ
            if (mean_importance > threshold and 
                std_importance < mean_importance * 0.5):  # –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å > 50%
                selected_features.append(col)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
        selected_features.sort(key=lambda x: perm_importance.importances_mean[features.columns.get_loc(x)], reverse=True)
        
        self.logger.info(f"‚úÖ –û—Ç–æ–±—Ä–∞–Ω–æ {len(selected_features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ {len(features.columns)}")
        return selected_features
    
    def _select_features_correlation(self, features: pd.DataFrame, target: pd.Series, 
                                   threshold: float) -> List[str]:
        """
        –û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ —Å —Ç–∞—Ä–≥–µ—Ç–æ–º
        
        Args:
            features: DataFrame —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
            target: Series —Å —Ç–∞—Ä–≥–µ—Ç–æ–º
            threshold: –ü–æ—Ä–æ–≥ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
            
        Returns:
            –°–ø–∏—Å–æ–∫ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        """
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º target –≤ Series –µ—Å–ª–∏ —ç—Ç–æ numpy –º–∞—Å—Å–∏–≤
        if isinstance(target, np.ndarray):
            target_series = pd.Series(target, index=features.index)
        else:
            target_series = target
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
        correlations = features.corrwith(target_series).abs()
        
        # –û—Ç–±–∏—Ä–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤—ã—à–µ –ø–æ—Ä–æ–≥–∞
        selected_features = correlations[correlations > threshold].index.tolist()
        
        self.logger.info(f"‚úÖ –û—Ç–æ–±—Ä–∞–Ω–æ {len(selected_features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ {len(features.columns)}")
        return selected_features
    
    def _select_features_mutual_info(self, features: pd.DataFrame, target: pd.Series, 
                                   threshold: float) -> List[str]:
        """
        –û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ mutual information
        
        Args:
            features: DataFrame —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
            target: Series —Å —Ç–∞—Ä–≥–µ—Ç–æ–º
            threshold: –ü–æ—Ä–æ–≥ mutual information
            
        Returns:
            –°–ø–∏—Å–æ–∫ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        """
        from sklearn.feature_selection import mutual_info_regression
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º mutual information
        mi_scores = mutual_info_regression(features, target, random_state=42)
        
        # –û—Ç–±–∏—Ä–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤—ã—à–µ –ø–æ—Ä–æ–≥–∞ –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
        feature_scores = list(zip(features.columns, mi_scores))
        selected_features = [feat for feat, score in feature_scores if score > threshold]
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏ (mutual information)
        if selected_features:
            selected_scores = [(feat, score) for feat, score in feature_scores if feat in selected_features]
            selected_scores.sort(key=lambda x: x[1], reverse=True)
            selected_features = [feat for feat, score in selected_scores]
        
        self.logger.info(f"‚úÖ –û—Ç–æ–±—Ä–∞–Ω–æ {len(selected_features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ {len(features.columns)}")
        return selected_features
    
    def _remove_correlated_features(self, features: pd.DataFrame, threshold: float = 0.95) -> pd.DataFrame:
        """
        –£–¥–∞–ª–µ–Ω–∏–µ —Å–∏–ª—å–Ω–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        
        Args:
            features: DataFrame —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
            threshold: –ü–æ—Ä–æ–≥ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
            
        Returns:
            DataFrame –±–µ–∑ –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        """
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É
        corr_matrix = features.corr().abs()
        
        # –ù–∞—Ö–æ–¥–∏–º –≤–µ—Ä—Ö–Ω–∏–π —Ç—Ä–µ—É–≥–æ–ª—å–Ω–∏–∫ –º–∞—Ç—Ä–∏—Ü—ã
        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
        
        # –ù–∞—Ö–æ–¥–∏–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
        to_drop = [column for column in upper.columns if any(upper[column] > threshold)]
        
        if to_drop:
            self.logger.info(f"   –£–¥–∞–ª—è–µ–º {len(to_drop)} –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
            return features.drop(columns=to_drop)
        
        return features
    
    def _select_features_combined(self, features: pd.DataFrame, target: pd.Series, 
                                threshold: float) -> List[str]:
        """
        –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤)
        
        Args:
            features: DataFrame —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
            target: Series —Å —Ç–∞—Ä–≥–µ—Ç–æ–º
            threshold: –ü–æ—Ä–æ–≥ –¥–ª—è –æ—Ç–±–æ—Ä–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        """
        self.logger.info("üîÑ –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤ —Å —Ç–∞–π–º–∞—É—Ç–æ–º –¥–ª—è –º–µ–¥–ª–µ–Ω–Ω—ã—Ö
        methods = ['correlation', 'mutual_info', 'permutation']  # correlation —Å–∞–º—ã–π –±—ã—Å—Ç—Ä—ã–π
        selected_sets = []
        
        for method in methods:
            try:
                self.logger.info(f"   –ó–∞–ø—É—Å–∫ –º–µ—Ç–æ–¥–∞ {method}...")
                
                if method == 'permutation':
                    # –î–ª—è permutation –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ
                    if len(features) > 50000:  # –ï—Å–ª–∏ –±–æ–ª—å—à–µ 50k —Å—Ç—Ä–æ–∫, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º permutation
                        self.logger.info(f"   –ü—Ä–æ–ø—É—Å–∫–∞–µ–º {method} (—Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –¥–∞–Ω–Ω—ã—Ö)")
                        continue
                    selected = self._select_features_permutation(features, target, threshold)
                elif method == 'correlation':
                    selected = self._select_features_correlation(features, target, threshold)
                elif method == 'mutual_info':
                    selected = self._select_features_mutual_info(features, target, threshold)
                
                selected_sets.append(set(selected))
                self.logger.info(f"   {method}: {len(selected)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
                
            except Exception as e:
                self.logger.warning(f"   –û—à–∏–±–∫–∞ –≤ –º–µ—Ç–æ–¥–µ {method}: {e}")
                continue
        
        if not selected_sets:
            return []
        
        # –ù–∞—Ö–æ–¥–∏–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤
        final_selected = set.intersection(*selected_sets)
        
        # –ï—Å–ª–∏ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–æ–µ, –±–µ—Ä–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ
        if len(final_selected) < 5:
            self.logger.info("   –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–æ–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ")
            final_selected = set.union(*selected_sets)
        
        return list(final_selected)
    
    def _select_features_recursive(self, features: pd.DataFrame, target: pd.Series, 
                                 n_features: int) -> List[str]:
        """
        –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –æ—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        
        Args:
            features: DataFrame —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
            target: Series —Å —Ç–∞—Ä–≥–µ—Ç–æ–º
            n_features: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ—Ç–±–æ—Ä–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        """
        from sklearn.feature_selection import RFE
        from sklearn.ensemble import RandomForestRegressor
        
        self.logger.info(f"üîÑ –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –æ—Ç–±–æ—Ä {n_features} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        
        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è RFE
        estimator = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
        
        # –°–æ–∑–¥–∞–µ–º —Å–µ–ª–µ–∫—Ç–æ—Ä
        selector = RFE(estimator, n_features_to_select=n_features, step=0.1)
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –æ—Ç–±–æ—Ä
        selector.fit(features, target)
        
        # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        selected_features = features.columns[selector.support_].tolist()
        
        return selected_features
    
    def _select_features_stability(self, features: pd.DataFrame, target: pd.Series, 
                                 threshold: float) -> List[str]:
        """
        –û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –∞–Ω–∞–ª–∏–∑–æ–º —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        
        Args:
            features: DataFrame —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
            target: Series —Å —Ç–∞—Ä–≥–µ—Ç–æ–º
            threshold: –ü–æ—Ä–æ–≥ –¥–ª—è –æ—Ç–±–æ—Ä–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Å—Ç–∞–±–∏–ª—å–Ω–æ –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        """
        from sklearn.model_selection import KFold
        from sklearn.inspection import permutation_importance
        from sklearn.ensemble import RandomForestRegressor
        
        self.logger.info("üîÑ –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        n_splits = 5
        n_repeats = 5
        
        # –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        feature_importance = {col: [] for col in features.columns}
        
        # K-fold –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
        kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)
        
        for fold, (train_idx, val_idx) in enumerate(kf.split(features)):
            X_train, X_val = features.iloc[train_idx], features.iloc[val_idx]
            y_train, y_val = target.iloc[train_idx], target.iloc[val_idx]
            
            # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
            model = RandomForestRegressor(n_estimators=100, random_state=42)
            model.fit(X_train, y_train)
            
            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º permutation importance
            perm_importance = permutation_importance(model, X_val, y_val, 
                                                   n_repeats=n_repeats, random_state=42)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞
            for i, col in enumerate(features.columns):
                feature_importance[col].append(perm_importance.importances_mean[i])
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ä–µ–¥–Ω—é—é –≤–∞–∂–Ω–æ—Å—Ç—å –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å
        feature_stats = {}
        for col, importances in feature_importance.items():
            mean_importance = np.mean(importances)
            std_importance = np.std(importances)
            stability = 1 - (std_importance / (mean_importance + 1e-8))  # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –≤–∞—Ä–∏–∞—Ü–∏–∏
            
            feature_stats[col] = {
                'mean_importance': mean_importance,
                'std_importance': std_importance,
                'stability': stability
            }
        
        # –û—Ç–±–∏—Ä–∞–µ–º —Å—Ç–∞–±–∏–ª—å–Ω–æ –≤–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        selected_features = []
        for col, stats in feature_stats.items():
            if (stats['mean_importance'] > threshold and 
                stats['stability'] > 0.5):  # –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å > 50%
                selected_features.append(col)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
        selected_features.sort(key=lambda x: feature_stats[x]['mean_importance'], reverse=True)
        
        self.logger.info(f"   –ù–∞–π–¥–µ–Ω–æ {len(selected_features)} —Å—Ç–∞–±–∏–ª—å–Ω–æ –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        
        return selected_features
    
    def clear_cache(self):
        """
        –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        """
        # –û—á–∏—â–∞–µ–º –∫—ç—à –≤ –ø–∞–º—è—Ç–∏
        self._memory_cache.clear()
        self._cache_metadata.clear()
        
        # –û—á–∏—â–∞–µ–º –∫—ç—à –Ω–∞ –¥–∏—Å–∫–µ
        for cache_file in self.cache_root.glob("*.pkl"):
            cache_file.unlink()
        
        self.logger.info("üóëÔ∏è –ö—ç—à –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –æ—á–∏—â–µ–Ω")
    
    def get_info(self) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–µ–Ω–µ–¥–∂–µ—Ä–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        """
        return {
            'cache_root': str(self.cache_root),
            'cache_enabled': self.config.features.cache_features,
            'validation_enabled': self.config.features.validate_features,
            'handle_inf_nan': self.config.features.handle_inf_nan,
            'memory_cache_size': len(self._memory_cache),
            'include_technical_indicators': self.config.features.include_technical_indicators,
            'include_multi_timeframe': self.config.features.include_multi_timeframe,
            'include_lag_features': self.config.features.include_lag_features,
            'include_rolling_features': self.config.features.include_rolling_features
        } 